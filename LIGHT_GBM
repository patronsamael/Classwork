import optuna
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load your dataset or create a sample dataset for demonstration
# Replace X and y with your features and target variables, respectively.
# X = ...
# y = ...

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the objective function for Optuna to optimize
def objective(trial):
    param = {
        "objective": "multiclass",
        "metric": "multi_logloss",
        "num_class": len(np.unique(y)),
        "boosting_type": "gbdt",
        "verbosity": -1,
        "n_jobs": -1,
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
        "num_leaves": trial.suggest_int("num_leaves", 2, 256),
        "max_depth": trial.suggest_int("max_depth", 3, 15),
        "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
        "subsample": trial.suggest_loguniform("subsample", 0.5, 1),
        "colsample_bytree": trial.suggest_loguniform("colsample_bytree", 0.5, 1),
    }

    # Create a Light GBM classifier
    lgb_model = lgb.LGBMClassifier(**param)

    # Train the model
    lgb_model.fit(X_train, y_train)

    # Predict on the validation set
    y_pred = lgb_model.predict(X_test)

    # Calculate the accuracy for the validation set
    accuracy = accuracy_score(y_test, y_pred)

    return accuracy

# Perform Bayesian optimization using Optuna with a higher number of trials
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=300)  # Increase the number of trials to 300

# Get the best parameters and best accuracy from the optimization
best_params = study.best_params
best_accuracy = study.best_value

print(f"Best Parameters: {best_params}")
print(f"Best Accuracy: {best_accuracy:.2f}")
